# ============================================================
# STREAMLIT ELECTRICAL DATA DASHBOARD ‚Äì CORRECTED & ROBUST
# - Explicit ID vs SMA source tagging
# - Validated measurement dictionary
# - Dictionary validation report panel
# - Safe plotting across mixed schemas
# - NO WARNINGS VERSION
# ============================================================

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import os
import glob
from datetime import datetime, timedelta, time

# ============================================================
# PAGE CONFIG
# ============================================================
st.set_page_config(page_title="UJA Electrical Data Dashboard", layout="wide")

# ============================================================
# MEASUREMENT DICTIONARY (SCHEMA-SAFE)
# ============================================================
MEASUREMENT_DICT = {
    "Voltage": {
        "AC": ["V_ID2", "V_ID3", "VAC_SMA"],
        "DC": ["V_ID1", "VDC_SMA"],
    },
    "Current": {
        "AC": ["I_ID2", "I_ID3", "I_SMA"],
        "DC": ["I_ID1", "IDC_SMA"],
    },
    "Power": {
        "AC": ["P_ID2", "P_ID3", "PAC_SMA"],
        "DC": ["Pot_ID1", "PDC_SMA"],
        "Reactive": ["Q_ID2", "Q_ID3"],
        "Apparent": ["S_ID2", "S_ID3"],
    },
    "Energy": {
        "AC": ["Etot_ID2", "Etot_ID3", "Ertot_ID2", "Ertot_ID3", "Etotal_SMA"],
        "DC": ["Eacumulada_ID1"],
    },
    "Frequency": {
        "AC": ["F_ID2", "F_SMA"],
    },
    "Power Factor": {
        "AC": ["Fdp_ID2", "Fdp_ID3"],
    },
    "Temperature": {
        "Sensor": ["T1_ID1", "T1_ID2", "Temp_SMA"],
    },
    "Solar Irradiance": {
        "Sensor": ["G_ID1"],
    },
}

# ============================================================
# UTILITIES
# ============================================================

def detect_source_type(columns):
    """Detect if data source is SMA inverter or ID-based sensor"""
    if any(c.endswith("_SMA") for c in columns):
        return "SMA"
    if any("_ID" in c for c in columns):
        return "ID"
    return "UNKNOWN"


def select_columns(df, measurement, subtype):
    """Returns all columns for the selected measurement that exist in df"""
    desired = MEASUREMENT_DICT[measurement][subtype]
    return [c for c in desired if c in df.columns]


def dictionary_validation(df):
    """Generate validation report showing expected vs found columns"""
    rows = []
    for meas, groups in MEASUREMENT_DICT.items():
        for group, cols in groups.items():
            found = [c for c in cols if c in df.columns]
            rows.append({
                "Measurement": meas,
                "Group": group,
                "Expected": ", ".join(cols),
                "Found": ", ".join(found) if found else "‚ùå None",
            })
    return pd.DataFrame(rows)


def parse_datetime_smart(fecha_str, hora_str):
    """Smart datetime parser that handles multiple formats without warnings"""
    combined = f"{fecha_str} {hora_str}"
    
    # Try common formats in order
    formats_to_try = [
        "%Y-%m-%d %H:%M:%S",
        "%d/%m/%Y %H:%M:%S",
        "%Y/%m/%d %H:%M:%S",
        "%d-%m-%Y %H:%M:%S",
    ]
    
    for fmt in formats_to_try:
        try:
            return pd.to_datetime(combined, format=fmt)
        except:
            continue
    
    # If all fail, return NaT
    return pd.NaT

# ============================================================
# DATA LOADING
# ============================================================
BASE_DATA_DIR = os.getenv(
    "STREAMLIT_DATA_DIR",
    "/var/www/uja_streamlit/data"
)
@st.cache_data
def load_data():
    """Load all CSV files from subdirectories with robust date parsing"""
    dfs = []
    for folder in os.listdir(BASE_DATA_DIR):
        folder_path = os.path.join(BASE_DATA_DIR, folder)
        if not os.path.isdir(folder_path):
            continue
        for file in glob.glob(os.path.join(folder_path, "*.csv")):
            try:
                df = pd.read_csv(file)
                if len(df.columns) == 1:
                    df = pd.read_csv(file, sep=';')
            except Exception:
                continue

            # Check for required columns (case-insensitive)
            fecha_col = next((c for c in df.columns if c.lower() == "fecha"), None)
            hora_col = next((c for c in df.columns if c.lower() == "hora"), None)
            
            if not fecha_col or not hora_col:
                continue

            # Smart datetime parsing without warnings
            df["DateTime"] = df.apply(
                lambda row: parse_datetime_smart(str(row[fecha_col]), str(row[hora_col])),
                axis=1
            )
            
            df = df.dropna(subset=["DateTime"])
            df = df.sort_values("DateTime")

            df["Source_House"] = folder
            df["Source_Type"] = detect_source_type(df.columns)
            dfs.append(df)

    if not dfs:
        return pd.DataFrame()
    return pd.concat(dfs, ignore_index=True)

# ============================================================
# LOAD DATA
# ============================================================
st.title("‚ö° Electrical Dashboard for UJA")

df = load_data()
if df.empty:
    st.error("No valid data found")
    st.stop()

# ============================================================
# SIDEBAR FILTERS
# ============================================================
with st.sidebar:
    st.header("üîß Filters")
    
    # Show data sources before filtering
    st.markdown("**Available Data Sources:**")
    for house in sorted(df["Source_House"].unique()):
        house_data = df[df["Source_House"] == house]
        source_type = house_data["Source_Type"].iloc[0] if len(house_data) > 0 else "Unknown"
        st.text(f"  ‚Ä¢ {house} ({source_type}): {len(house_data):,} rows")
    
    houses = sorted(df["Source_House"].unique())
    selected_houses = st.multiselect("House", houses, default=houses)
    df = df[df["Source_House"].isin(selected_houses)]

    st.markdown("**Date Range in Data:**")
    st.text(f"  From: {df.DateTime.min().date()}")
    st.text(f"  To: {df.DateTime.max().date()}")
    
    date_range = st.date_input("Date range", [df.DateTime.min().date(), df.DateTime.max().date()])
    
    # Handle single date or date range
    if isinstance(date_range, tuple) and len(date_range) == 2:
        start_date, end_date = date_range
    elif isinstance(date_range, (list, tuple)) and len(date_range) == 1:
        start_date = end_date = date_range[0]
    else:
        start_date = end_date = date_range
    
    df = df[(df.DateTime.dt.date >= start_date) & (df.DateTime.dt.date <= end_date)]
    
    # Time slicer
    st.markdown("---")
    st.markdown("### ‚è∞ Time of Day Filter")
    
    time_filter_enabled = st.checkbox("Enable time filter", value=False)
    
    if time_filter_enabled:
        time_range = st.slider(
            "Select hours",
            min_value=0,
            max_value=23,
            value=(0, 23),
            step=1,
            format="%d:00"
        )
        
        start_hour, end_hour = time_range
        
        # Filter by time
        df = df[
            (df['DateTime'].dt.hour >= start_hour) & 
            (df['DateTime'].dt.hour <= end_hour)
        ]
        
        st.info(f"üïê Time filter: {start_hour}:00 - {end_hour}:00")
    
    st.markdown("---")
    st.info(f"üìä Filtered: {len(df):,} rows")
    
    if len(df) == 0:
        st.error("‚ö†Ô∏è No data after filtering! Try expanding date range.")

# ============================================================
# DICTIONARY VALIDATION PANEL
# ============================================================
with st.expander("üß™ Dictionary Validation Report", expanded=False):
    st.markdown("**This report shows which columns from the measurement dictionary exist in your current dataset:**")
    report = dictionary_validation(df)
    st.dataframe(report, width='stretch')

# Show all available columns for debugging
with st.expander("üîç Debug: All Available Columns in Current Dataset", expanded=False):
    all_cols = sorted([col for col in df.columns if col not in ['DateTime', 'Source_House', 'Source_Type']])
    st.markdown(f"**Total columns: {len(all_cols)}**")
    
    # Separate ID and SMA columns
    id_cols = [c for c in all_cols if '_ID' in c]
    sma_cols = [c for c in all_cols if '_SMA' in c]
    other_cols = [c for c in all_cols if '_ID' not in c and '_SMA' not in c]
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**ID Columns:**")
        st.write(id_cols if id_cols else "None")
    with col2:
        st.markdown("**SMA Columns:**")
        st.write(sma_cols if sma_cols else "None")
    with col3:
        st.markdown("**Other Columns:**")
        st.write(other_cols if other_cols else "None")

# ============================================================
# MEASUREMENT SELECTION
# ============================================================
st.markdown("### üìä Select Measurement to Visualize")

measurement = st.selectbox("Measurement Type", list(MEASUREMENT_DICT.keys()))

# Only show types that have available data
types_available = [
    t for t in MEASUREMENT_DICT[measurement]
    if any(c in df.columns for c in MEASUREMENT_DICT[measurement][t])
]

if not types_available:
    st.warning(f"‚ö†Ô∏è No data available for {measurement} in current dataset.")
    st.stop()

subtype = st.radio("Type", types_available, horizontal=True)
columns = select_columns(df, measurement, subtype)

if not columns:
    st.warning("No columns available for this selection")
    st.stop()

st.success(f"‚úÖ Plotting {len(columns)} columns: {', '.join(columns)}")

# Show data availability for each column
with st.expander("üìä Data Availability Check", expanded=False):
    for col in columns:
        col_data = df[col].dropna()
        total_rows = len(df)
        non_null_rows = len(col_data)
        null_rows = total_rows - non_null_rows
        
        st.markdown(f"**{col}:**")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Total Rows", f"{total_rows:,}")
        c2.metric("Non-Null", f"{non_null_rows:,}")
        c3.metric("Null/Empty", f"{null_rows:,}")
        c4.metric("% Available", f"{(non_null_rows/total_rows*100):.1f}%")
        
        # Show sample values if available
        if non_null_rows > 0:
            st.write(f"Sample values: {col_data.head(5).tolist()}")
        else:
            st.warning(f"‚ö†Ô∏è All values are NULL or empty for {col}")
        st.markdown("---")

# ============================================================
# TABS
# ============================================================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä KPIs", 
    "üìà Time Series", 
    "üìä Histogram", 
    "üîµ Scatter Plots",
    "ü•ß Pie Chart"
])

# ============================================================
# TAB 1 ‚Äì KPIs
# ============================================================
with tab1:
    st.subheader("Key Performance Indicators")
    
    for col in columns:
        st.markdown(f"#### üîπ {col}")
        col_data = df[col].dropna()
        if col_data.empty:
            st.warning(f"No data for {col}")
            continue
        
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Mean", f"{col_data.mean():.2f}")
        c2.metric("Min", f"{col_data.min():.2f}")
        c3.metric("Max", f"{col_data.max():.2f}")
        c4.metric("Sum", f"{col_data.sum():.2f}")
        st.markdown("---")

# ============================================================
# TAB 2 ‚Äì TIME SERIES
# ============================================================
with tab2:
    st.subheader(f"{measurement} ({subtype}) ‚Äì Time Series")
    
    fig = go.Figure()
    for col in columns:
        temp = df[["DateTime", col]].dropna()
        if temp.empty:
            continue
        temp = temp.set_index("DateTime")[col].resample("5min").mean().reset_index()
        fig.add_trace(go.Scatter(
            x=temp.DateTime, 
            y=temp[col], 
            mode="lines+markers", 
            name=col,
            marker=dict(size=4)
        ))
    
    fig.update_layout(
        height=500, 
        template="plotly_white", 
        hovermode="x unified",
        xaxis_title="DateTime",
        yaxis_title=measurement
    )
    st.plotly_chart(fig, width='stretch')

# ============================================================
# TAB 3 ‚Äì HISTOGRAM
# ============================================================
with tab3:
    st.subheader(f"{measurement} ({subtype}) ‚Äì Histogram Over Time")
    
    # Create time-based histogram (bins by date)
    fig_hist = go.Figure()
    
    for col in columns:
        temp = df[["DateTime", col]].dropna()
        if temp.empty:
            continue
        
        # Resample to hourly or daily bins depending on date range
        date_diff = (temp['DateTime'].max() - temp['DateTime'].min()).days
        
        if date_diff <= 7:
            # For week or less: hourly bins
            temp_resampled = temp.set_index("DateTime")[col].resample("1h").mean().reset_index()
            bin_label = "Hourly"
        elif date_diff <= 31:
            # For month or less: 6-hourly bins
            temp_resampled = temp.set_index("DateTime")[col].resample("6h").mean().reset_index()
            bin_label = "6-Hourly"
        else:
            # For longer periods: daily bins
            temp_resampled = temp.set_index("DateTime")[col].resample("1D").mean().reset_index()
            bin_label = "Daily"
        
        fig_hist.add_trace(
            go.Bar(
                x=temp_resampled['DateTime'],
                y=temp_resampled[col],
                name=col,
                opacity=0.7
            )
        )
    
    fig_hist.update_layout(
        height=500,
        template="plotly_white",
        barmode='group',
        xaxis_title="DateTime",
        yaxis_title=measurement,
        title=f"{measurement} ({subtype}) ‚Äì {bin_label} Average"
    )
    st.plotly_chart(fig_hist, width='stretch')

# ============================================================
# TAB 4 ‚Äì SCATTER PLOTS
# ============================================================
with tab4:
    st.subheader(f"{measurement} ({subtype}) ‚Äì Scatter Plots Over Time")
    
    for col in columns:
        st.markdown(f"### üîπ Scatter Plot for: `{col}`")
        
        temp = df[['DateTime', col]].dropna()
        if temp.empty:
            st.warning(f"No data for {col}")
            continue
        
        fig_scatter = px.scatter(
            temp,
            x='DateTime',
            y=col,
            title=f"{col} Over Time",
            color_discrete_sequence=["#1f77b4"]
        )
        fig_scatter.update_traces(mode='markers', marker=dict(size=6))
        fig_scatter.update_layout(
            height=400,
            template='plotly_white',
            xaxis_title='DateTime',
            yaxis_title=col,
            hovermode='x unified'
        )
        
        st.plotly_chart(fig_scatter, width='stretch')

# ============================================================
# TAB 5 ‚Äì PIE CHART
# ============================================================
with tab5:
    st.subheader(f"{measurement} ({subtype}) ‚Äì Total Contribution by Column")
    
    # Calculate total sum for each column
    totals = []
    for col in columns:
        col_sum = df[col].sum()
        if col_sum > 0:
            totals.append({"Column": col, "Total": col_sum})
    
    if not totals:
        st.warning("No positive values found")
    else:
        df_totals = pd.DataFrame(totals)
        
        fig_pie = px.pie(
            df_totals,
            values="Total",
            names="Column",
            title=f"{measurement} ({subtype}) ‚Äì Total Contribution",
            hole=0.3
        )
        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
        fig_pie.update_layout(height=500, template="plotly_white")
        st.plotly_chart(fig_pie, width='stretch')
